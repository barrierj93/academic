{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b348631",
   "metadata": {},
   "source": [
    "\n",
    "# ETL Process: Data Extraction, Transformation, and Loading\n",
    "\n",
    "This notebook implements an **ETL (Extract, Transform, Load) pipeline**, which is commonly used for processing structured datasets. The key objectives of this notebook are:\n",
    "\n",
    "1. **Data Extraction**: Loading raw data from various sources (CSV, databases, APIs).\n",
    "2. **Data Transformation**: Cleaning, normalizing, and restructuring the data.\n",
    "3. **Data Loading**: Storing the processed data in a suitable format (database, CSV, or other storage).\n",
    "\n",
    "## Summary of Assets:\n",
    "\n",
    "- **Input Data**: Raw datasets extracted from different sources.\n",
    "- **Transformation Steps**: Data cleaning, normalization, type conversion, handling missing values.\n",
    "- **Output Data**: Processed and structured data ready for analysis or storage.\n",
    "\n",
    "Below, the code is interleaved with explanatory Markdown sections to clarify each step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7a6ba9",
   "metadata": {},
   "source": [
    "**Processing Step**: This section performs data operations as part of the ETL pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Script tonto para eliminar caracteres sobrantes del \n",
    "# # archivo csv (Draft_AnonymousAttacks.csv)\n",
    "\n",
    "import csv\n",
    "import re\n",
    "\n",
    "def clean_csv(input_path, output_path):\n",
    "    with open(input_path, 'r', newline='', encoding='utf-8') as infile, \\\n",
    "         open(output_path, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        reader = csv.reader(infile)\n",
    "        writer = csv.writer(outfile)\n",
    "\n",
    "        for row in reader:\n",
    "#             # Eliminar todos los corchetes con números dentro en cada columna\n",
    "            cleaned_row = [re.sub(r'\\s*\\[\\d+\\]\\s*', '', cell) for cell in row]\n",
    "            writer.writerow(cleaned_row)\n",
    "\n",
    "def main():\n",
    "    input_path = input(\"Ingrese la ruta del archivo de entrada: \")\n",
    "    output_folder = input(\"Ingrese la ruta de la carpeta de salida: \")\n",
    "    output_filename = input(\"Ingrese el nombre del archivo de salida (sin extensión): \")\n",
    "    \n",
    "    output_path = f\"{output_folder}/{output_filename}.csv\"\n",
    "\n",
    "    clean_csv(input_path, output_path)\n",
    "    print(f\"Archivo limpio guardado en: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f4a70c",
   "metadata": {},
   "source": [
    "**Library Imports**: This section loads essential Python libraries for data manipulation and ETL tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "def leer_diccionario(ruta_diccionario):\n",
    "    with open(ruta_diccionario, 'r', encoding='utf-8') as f:\n",
    "        return [linea.strip() for linea in f if linea.strip()]\n",
    "\n",
    "def buscar_coincidencias(fila, diccionario):\n",
    "    return '; '.join(palabra for palabra in diccionario if palabra.lower() in fila.lower())\n",
    "\n",
    "def procesar_csv(ruta_entrada, ruta_salida, diccionario, nombre_columna):\n",
    "    with open(ruta_entrada, 'r', newline='', encoding='utf-8') as archivo_entrada, \\\n",
    "         open(ruta_salida, 'w', newline='', encoding='utf-8') as archivo_salida:\n",
    "        \n",
    "        lector = csv.reader(archivo_entrada, delimiter=';')\n",
    "        escritor = csv.writer(archivo_salida, delimiter=';')\n",
    "        \n",
    "        encabezados = next(lector)\n",
    "        encabezados.append(nombre_columna)\n",
    "        escritor.writerow(encabezados)\n",
    "        \n",
    "        for fila in lector:\n",
    "            coincidencias = buscar_coincidencias(';'.join(fila), diccionario)\n",
    "            fila.append(coincidencias)\n",
    "            escritor.writerow(fila)\n",
    "\n",
    "def main():\n",
    "    ruta_diccionario = input(\"Introduce la ruta del archivo de diccionario (.txt): \")\n",
    "    ruta_csv_entrada = input(\"Introduce la ruta del archivo CSV de entrada: \")\n",
    "    ruta_salida = input(\"Introduce la ruta para el archivo de salida: \")\n",
    "    nombre_columna = input(\"Introduce el nombre para la nueva columna: \")\n",
    "    nombre_archivo_salida = input(\"Introduce el nombre para el archivo de salida (sin .csv): \")\n",
    "    \n",
    "    ruta_csv_salida = os.path.join(ruta_salida, f\"{nombre_archivo_salida}.csv\")\n",
    "    \n",
    "    diccionario = leer_diccionario(ruta_diccionario)\n",
    "    procesar_csv(ruta_csv_entrada, ruta_csv_salida, diccionario, nombre_columna)\n",
    "    \n",
    "    print(f\"Proceso completado. El archivo de salida se encuentra en: {ruta_csv_salida}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9558e5",
   "metadata": {},
   "source": [
    "**Processing Step**: This section performs data operations as part of the ETL pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Web Scraping (wikipedia) para obtener un csv con\n",
    "# # los datos necesitados del grupo Anonymous\n",
    "\n",
    "\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# # Pedir la URL y el nombre del archivo CSV\n",
    "url_html = input(\"Ingresa la URL del archivo HTML: \")\n",
    "ruta_csv = input(\"Ingresa la ruta donde se guardará el archivo CSV: \")\n",
    "nombre_csv = input(\"Ingresa el nombre del archivo CSV (sin extensión): \")\n",
    "\n",
    "# # Realizar la petición HTTP para obtener el contenido del HTML\n",
    "response = requests.get(url_html)\n",
    "response.raise_for_status()  # Levanta un error si la petición no es exitosa\n",
    "\n",
    "# # Analizar el contenido HTML con BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# # Crear una lista para almacenar los datos\n",
    "data = []\n",
    "\n",
    "# # Variables para almacenar el año y el título actuales\n",
    "current_year = \"\"\n",
    "current_title = \"\"\n",
    "\n",
    "# # Buscar todos los elementos <div> con clase mw-heading\n",
    "sections = soup.find_all('div', class_='mw-heading')\n",
    "\n",
    "for section in sections:\n",
    "    if 'mw-heading2' in section['class']:\n",
    "#         # Es un nuevo año\n",
    "        current_year = section.get_text(strip=True)\n",
    "        current_title = \"\"  # Resetear el título para el caso donde no haya\n",
    "        sibling = section.find_next_sibling()\n",
    "        \n",
    "#         # Manejar casos antes de 2011 donde el cuerpo del incidente está dentro de <li>\n",
    "        if sibling and sibling.name == 'ul':\n",
    "            for li in sibling.find_all('li'):\n",
    "                description = li.get_text(strip=True)\n",
    "                data.append([current_year, \"Sin título\", description])\n",
    "                \n",
    "    elif 'mw-heading3' in section['class']:\n",
    "#         # Es un nuevo incidente con título\n",
    "        current_title = section.get_text(strip=True)\n",
    "        description = \"\"\n",
    "        sibling = section.find_next_sibling()\n",
    "        \n",
    "#         # Recorrer los hermanos siguientes para construir la descripción\n",
    "        while sibling and not any('mw-heading' in cls for cls in sibling.get('class', [])):\n",
    "            if sibling.name == 'p':\n",
    "#                 # Manipular <a href> dentro de <p> para añadir espacios antes y después\n",
    "                for a in sibling.find_all('a'):\n",
    "                    a.insert_before(' ')\n",
    "                    a.insert_after(' ')\n",
    "                description += sibling.get_text() + \" \"  # Eliminar strip() para preservar los espacios\n",
    "            sibling = sibling.find_next_sibling()\n",
    "\n",
    "#         # Limpiar y agregar los datos a la lista\n",
    "        description = description.strip()\n",
    "        if current_year and description:\n",
    "            data.append([current_year, current_title, description])\n",
    "\n",
    "# # Guardar los datos en un archivo CSV usando punto y coma como delimitador\n",
    "csv_file = f\"{ruta_csv}/{nombre_csv}.csv\"\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file, delimiter=';', quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "    writer.writerow(['Año', 'Título', 'Descripción'])\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(f\"Archivo CSV guardado en: {csv_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
